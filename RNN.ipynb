{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy2_q1yA_eY2"
      },
      "source": [
        "# Recurrent Neural Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFWSw3mPzCd3"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePyjL-fEA4As",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8887d716-2949-4621-f4cd-1d434d4afcb4"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# for reproducibility\n",
        "torch.manual_seed(100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f50b8ecd610>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-W8ykrC5Pz4"
      },
      "source": [
        "### 1 Dataset Construction\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh5IsVGt5ba4",
        "outputId": "6c3e0f44-584a-47f7-c5c9-3d1b5eeb6dce"
      },
      "source": [
        "# Dictionary\n",
        "\n",
        "sample_sentence = 'hi!hello.'\n",
        "# 학습할 데이터 sample_sentence에 저장\n",
        "char_set = list(set(sample_sentence))\n",
        "# 중복된 값을 제거한 list를 생성해 char_set에 저장\n",
        "# -> [h,i,!,e,l,o,.]\n",
        "dic = {c: i for i, c in enumerate(char_set)}\n",
        "# enumerate함수를 사용해 char_set의 각각의 value값과 index값을 딕셔너리 함수에 넣어준다\n",
        "# -> {h:0, i:1, !:2, e:3, l:4, o:5, .:6}\n",
        "print(char_set)\n",
        "\n",
        "# Parameters\n",
        "\n",
        "dic_size = len(dic)\n",
        "# dic의 길이만큼 dic_size에 대입 -> 7\n",
        "input_size = dic_size\n",
        "# dic_size로 input_zise 결정 -> 7\n",
        "hidden_size = dic_size\n",
        "# dic_size로 hidden_size 결정 -> 7\n",
        "\n",
        "\n",
        "# Dataset setting\n",
        "\n",
        "x_batch = []\n",
        "# 빈 리스트 생성해 x_batch에 저장\n",
        "y_batch = []\n",
        "# 빈 리스트 생성해 y_batch에 저장\n",
        "\n",
        "x_data = [dic[c] for c in sample_sentence[:-1]]\n",
        "# 마지막 원소(.)을 제외한 값(input)을 위에 만들어 놓은 dic의 value값에 각각 대입\n",
        "# -> x_data= [0, 1, 2, 0, 3, 4, 4, 5]\n",
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
        "# embedding 과정 ->\n",
        "# x_one_hot =\n",
        "# h = [1 0 0 0 0 0 0]\n",
        "# i = [0 1 0 0 0 0 0]\n",
        "# ! = [0 0 1 0 0 0 0]\n",
        "# h = [1 0 0 0 0 0 0]\n",
        "# e = [0 0 0 1 0 0 0]\n",
        "# l = [0 0 0 0 1 0 0]\n",
        "# l = [0 0 0 0 1 0 0]\n",
        "# o = [0 0 0 0 0 1 0]\n",
        "# 이것이 one_hot encoding\n",
        "y_data = [dic[c] for c in sample_sentence[1:]]\n",
        "# sample_date의 [1]부터 모든 원소(output)를 위에 만들어 놓은 dic의 value값에 각각 대입\n",
        "# y_data = [i=1, !=2, h=0, e=3, l=4, l=4, o=5, .=6]\n",
        "\n",
        "x_batch.append(x_one_hot)\n",
        "# x_one_hot 결과를 x_batch의 list에 넣어준다\n",
        "y_batch.append(y_data)\n",
        "# y_data의 결과를 y_batch의 list에 넣어준다\n",
        "\n",
        "\n",
        "# To torch tensors\n",
        "X = torch.FloatTensor(x_batch)\n",
        "# x_batch의 tensor값을 X에 대입 -> 배치사이즈(N) = 1, seqence size(S) = 8, Embedding size(E) = 7\n",
        "Y = torch.LongTensor(y_batch)\n",
        "# y_batch의 tensor값을 Y에 대입 -> 배치사이즈(N) = 1, hiddne size(H) = 8\n",
        "print(X.shape)\n",
        "# X의 형태 출력\n",
        "print(Y.shape)\n",
        "# Y의 형태 출력"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['!', '.', 'i', 'h', 'o', 'e', 'l']\n",
            "torch.Size([1, 8, 7])\n",
            "torch.Size([1, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SH2RX5pEBB0H"
      },
      "source": [
        "### 2 RNN model\n",
        "* Input (입력의 형태)\n",
        "  + Input type: torch.Tensor\n",
        "  + Input shape: (N x S x E)\n",
        "    - N: Batch size, S: Sequence length, E: Embedding size\n",
        "    - 단, **batch_first=True** 일 때\n",
        "  + 입력값 'hi!hello'\n",
        "    - (1, 8, 7)\n",
        "* Hidden (출력의 형태)\n",
        "  + Hidden type: torch.Tensor\n",
        "  + 출력값 'i!hello.'\n",
        "    - (1, 8, 7)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGu6jWggenAc"
      },
      "source": [
        "# Model\n",
        "learning_rate = 0.1\n",
        "# learning_rate를 0.1로 설정\n",
        "training_epochs = 60\n",
        "# epoch값 60 설정\n",
        "model = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "# 앞에 선언한 각각의 값들을 RNN모델에 넣는다. 여기서 batch_first=True로 지정한 이유는 앞의 형태가 (N x S x E)로 되어 있기 때문이다."
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6XeYPhI6Tig",
        "outputId": "3e0ef364-a20a-431d-83c7-84a49564afab"
      },
      "source": [
        "# define cost/loss & optimizer\n",
        "\n",
        "# 여기부터 전에 다뤄왔던 loss구하는 방법과 동일\n",
        "criterion = nn.CrossEntropyLoss()    # Softmax\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# train\n",
        "for epoch in range(training_epochs):\n",
        "  optimizer.zero_grad()\n",
        "  outputs, _status = model(X)\n",
        "  loss = criterion(outputs.reshape(-1, dic_size), Y.reshape(-1))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  if epoch % 5 == 4:\n",
        "    result = outputs.data.numpy().argmax(axis=2)\n",
        "    result_str = ''.join([char_set[c] for c in np.squeeze(result)])\n",
        "    print('epoch: ',epoch, 'loss: ', loss.item(), 'prediction: ', result_str, 'true Y: ', sample_sentence[1:])\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  4 loss:  1.2104920148849487 prediction:  .!hello. true Y:  i!hello.\n",
            "epoch:  9 loss:  0.8444290161132812 prediction:  i!hello. true Y:  i!hello.\n",
            "epoch:  14 loss:  0.7109715938568115 prediction:  i!hello. true Y:  i!hello.\n",
            "epoch:  19 loss:  0.6631023287773132 prediction:  i!hello. true Y:  i!hello.\n",
            "epoch:  24 loss:  0.6405735015869141 prediction:  i!hello. true Y:  i!hello.\n",
            "epoch:  29 loss:  0.6292124390602112 prediction:  i!hello. true Y:  i!hello.\n",
            "epoch:  34 loss:  0.6235072016716003 prediction:  i!hello. true Y:  i!hello.\n",
            "epoch:  39 loss:  0.6202727556228638 prediction:  i!hello. true Y:  i!hello.\n",
            "epoch:  44 loss:  0.6180737018585205 prediction:  i!hello. true Y:  i!hello.\n",
            "epoch:  49 loss:  0.6165153384208679 prediction:  i!hello. true Y:  i!hello.\n",
            "epoch:  54 loss:  0.615384578704834 prediction:  i!hello. true Y:  i!hello.\n",
            "epoch:  59 loss:  0.6144909858703613 prediction:  i!hello. true Y:  i!hello.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc6TPzluWB1M"
      },
      "source": [
        "### 3 Assignment\n",
        "### 다음 3개의 문장을 batch data로 활용해 RNN을 학습해보자 (아래의 미완성 코드, 위 실습코드, 실행결과를 참고)\n",
        "* 'howareyou'\n",
        "* 'whats up?'\n",
        "* 'iamgreat.'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "klzzEMCU9SuA",
        "outputId": "c26c647f-e932-485e-96e9-b9a451b2e9f8"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# for reproducibility\n",
        "torch.manual_seed(100)\n",
        "\n",
        "# Dictionary\n",
        "sample_sentences = ['howareyou', 'whats up?', 'iamgreat.']\n",
        "char_set = list(set(''.join(sample_sentences)))\n",
        "# 공백을 기준으로 모든 문자열을 하나로 합쳐 list를 만들어 준다.\n",
        "dic = {c: i for i, c in enumerate(char_set)}\n",
        "# 각각의 value와 index값을 딕셔너리에 넣어주고 그 값을 dic변수에 저장\n",
        "print(char_set)\n",
        "# char_set이 원하는 형태로 나오고 있는지 확인하기 위해 중간에 print함수를 넣어주었다.\n",
        "\n",
        "# Parameters\n",
        "dic_size = len(dic)\n",
        "input_size = dic_size\n",
        "hidden_size = dic_size\n",
        "\n",
        "# Dataset setting\n",
        "input_batch = []\n",
        "target_batch = []\n",
        "\n",
        "for sentence in sample_sentences:\n",
        "# 각 문장 3개를 반복문을 통해 진행\n",
        "    x_data = [dic[c] for c in sentence[:-1]]\n",
        "    x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
        "    y_data = [dic[c] for c in sentence[1:]]\n",
        "    input_batch.append(x_one_hot)\n",
        "    target_batch.append(y_data)\n",
        "\n",
        "# To torch tensors\n",
        "X = torch.FloatTensor(input_batch)\n",
        "Y = torch.LongTensor(target_batch)\n",
        "\n",
        "# Model\n",
        "learning_rate = 0.05\n",
        "training_epochs = 500\n",
        "model = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "criterion = nn.CrossEntropyLoss()    # Softmax\n",
        "# Cross-entropy loss는 one-hot 인코딩된 벡터와 직접적으로 비교될 필요 없이 정수 형태의 레이블을 기대한다.\n",
        "# 따라서 y_data는 각 문자에 대한 인덱스만 포함하면 된다\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# train\n",
        "for epoch in range(training_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs, _status = model(X)\n",
        "    loss = criterion(outputs.reshape(-1, dic_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        result = outputs.data.numpy().argmax(axis=2)\n",
        "        for sentence in result:\n",
        "            decoded_sentence = ''.join([char_set[c] for c in sentence])\n",
        "            print(decoded_sentence)\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['s', 'e', 'm', '.', ' ', 'o', 'h', 'a', 'u', 'p', 'i', 'y', 'w', 'g', 't', '?', 'r']\n",
            "emet s h\n",
            "eehmee  \n",
            "eh h spm\n",
            "owareaou\n",
            "hats up?\n",
            "amgreat.\n",
            "owareaou\n",
            "hats up?\n",
            "amgreat.\n",
            "owareaou\n",
            "hats up?\n",
            "amgreat.\n",
            "owareaou\n",
            "hats up?\n",
            "amgreat.\n"
          ]
        }
      ]
    }
  ]
}
